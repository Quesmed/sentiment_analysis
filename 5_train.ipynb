{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset from HF\n",
    "Loading from Quesmed organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['id', 'createdAt', 'userId', 'userCreatedAt', 'classYear', 'universityId', 'country', 'universityName', 'parentId', 'questionId', 'comment', 'review', 'negative', 'neutral', 'positive', 'tone', 'sadness', 'joy', 'love', 'anger', 'fear', 'surprise', 'emotion', 'educational', 'giving feedback', 'asking a question', 'insulting', 'supporting', 'humour', 'frustration', 'theme'],\n",
       "        num_rows: 15\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['id', 'createdAt', 'userId', 'userCreatedAt', 'classYear', 'universityId', 'country', 'universityName', 'parentId', 'questionId', 'comment', 'review', 'negative', 'neutral', 'positive', 'tone', 'sadness', 'joy', 'love', 'anger', 'fear', 'surprise', 'emotion', 'educational', 'giving feedback', 'asking a question', 'insulting', 'supporting', 'humour', 'frustration', 'theme'],\n",
       "        num_rows: 120\n",
       "    })\n",
       "    validate: Dataset({\n",
       "        features: ['id', 'createdAt', 'userId', 'userCreatedAt', 'classYear', 'universityId', 'country', 'universityName', 'parentId', 'questionId', 'comment', 'review', 'negative', 'neutral', 'positive', 'tone', 'sadness', 'joy', 'love', 'anger', 'fear', 'surprise', 'emotion', 'educational', 'giving feedback', 'asking a question', 'insulting', 'supporting', 'humour', 'frustration', 'theme'],\n",
       "        num_rows: 15\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset('quesmed/comment_sentiment', token=True)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "def isolate_dataset(ds: Dataset, feature: str):\n",
    "    cols = ds.column_names['train']\n",
    "    col_keep = {'comment', feature}\n",
    "    \n",
    "    ds_filter = ds.remove_columns(col_keep.symmetric_difference(cols))\n",
    "    ds_filter = ds_filter.rename_column(feature, 'label')\n",
    "    ds_filter = ds_filter.class_encode_column('label')\n",
    "\n",
    "    return ds_filter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup model and trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "def init_model(model_path: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    config = AutoConfig.from_pretrained(model_path)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "    return (tokenizer, config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, logging\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "def setup_trainer(name: str, dataset: Dataset, model, tokenizer):\n",
    "    model_name = f\"fine-tuning-chkp/{name}\"\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=model_name,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=0.2,\n",
    "        num_train_epochs=5,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        load_best_model_at_end=True,\n",
    "        disable_tqdm=False,\n",
    "        use_mps_device=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset['train'],\n",
    "        eval_dataset=dataset['validate'],\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tone_tokenizer, tone_config, tone_model = init_model(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86d445ff35c64db6ae1dd962d47731e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba8c93eed5154721a630401746126c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf1d429dea64b52911889688f734b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e45f644c41c34d438334f2c32e9fc214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd5e4bec84244adabbc209de0a42f6ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "888e8448d5464ef8ba5210be903bf7a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'label': ClassLabel(names=['negative', 'neutral', 'positive'], id=None),\n",
       " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_tone = isolate_dataset(ds, 'tone')\n",
    "\n",
    "ds_tone = ds_tone.map(\n",
    "  lambda row: tone_tokenizer(row['comment'], max_length=512, padding='max_length', truncation=True, return_tensors='pt'), \n",
    "  batched=True,\n",
    "  remove_columns=['comment']\n",
    ")\n",
    "\n",
    "ds_tone['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefan/.local/share/virtualenvs/sentiment_analysis-C3cLsf5i/lib/python3.11/site-packages/transformers/training_args.py:1759: UserWarning: `use_mps_device` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers.`mps` device will be used by default if available similar to the way `cuda` device is used.Therefore, no action from user is required. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tone_trainer = setup_trainer('tone', dataset=ds_tone, model=tone_model, tokenizer=tone_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefan/.local/share/virtualenvs/sentiment_analysis-C3cLsf5i/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "501189cbf76246e9a8676f58594fa8d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7455, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca73206df35a4aac91854ef2e52e32fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.514501690864563, 'eval_accuracy': 0.7333333333333333, 'eval_runtime': 0.525, 'eval_samples_per_second': 28.572, 'eval_steps_per_second': 3.81, 'epoch': 1.0}\n",
      "{'loss': 0.3538, 'learning_rate': 1.2e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb8c5f44c3d45a8ac5461c384c4677b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5528842806816101, 'eval_accuracy': 0.7333333333333333, 'eval_runtime': 0.3137, 'eval_samples_per_second': 47.823, 'eval_steps_per_second': 6.376, 'epoch': 2.0}\n",
      "{'loss': 0.1785, 'learning_rate': 8.000000000000001e-06, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a8a403cabe949b09af17394a457bf94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6121124625205994, 'eval_accuracy': 0.8, 'eval_runtime': 0.3222, 'eval_samples_per_second': 46.561, 'eval_steps_per_second': 6.208, 'epoch': 3.0}\n",
      "{'loss': 0.0672, 'learning_rate': 4.000000000000001e-06, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e2312572ac4ee08a528e5879021368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.673171877861023, 'eval_accuracy': 0.8666666666666667, 'eval_runtime': 0.3303, 'eval_samples_per_second': 45.409, 'eval_steps_per_second': 6.054, 'epoch': 4.0}\n",
      "{'loss': 0.0508, 'learning_rate': 0.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a457d87d68b844958ea08be18410b76c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6994945406913757, 'eval_accuracy': 0.8666666666666667, 'eval_runtime': 0.3382, 'eval_samples_per_second': 44.357, 'eval_steps_per_second': 5.914, 'epoch': 5.0}\n",
      "{'train_runtime': 70.4994, 'train_samples_per_second': 8.511, 'train_steps_per_second': 1.064, 'train_loss': 0.27914314905802406, 'epoch': 5.0}\n",
      "Time: 70.50\n",
      "Samples/second: 8.51\n"
     ]
    }
   ],
   "source": [
    "result = tone_trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fine-tuning-chkp/tone/checkpoint-60'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tone_trainer.state.best_model_checkpoint)\n",
    "tone_trainer.save_model('fine-tuning-final/tone')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_tokenizer, emotion_config, emotion_model = init_model(\"bhadresh-savani/distilbert-base-uncased-emotion\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d31322a95d432da5fb2a94e2f660d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ce8bc2b04b4206a77893aaca1fcc3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "307fe398a53945a5800e449d30a3abcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05798dff619141969e9954f2ab3ed0c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b69744144a4f4c229d7331fcb6142684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7341385dad0d4e26beef2121083a697f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'label': ClassLabel(names=['?puzzled', 'anger', 'fear', 'joy', 'sadness', 'surprise'], id=None),\n",
       " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# emotion_labels = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
    "\n",
    "ds_emotion = isolate_dataset(ds, 'emotion')\n",
    "\n",
    "ds_emotion = ds_emotion.map(\n",
    "  lambda row: emotion_tokenizer(row['comment'], max_length=512, padding='max_length', truncation=True, return_tensors='pt'), \n",
    "  batched=True,\n",
    "  remove_columns=['comment']\n",
    ")\n",
    "\n",
    "ds_emotion['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefan/.local/share/virtualenvs/sentiment_analysis-C3cLsf5i/lib/python3.11/site-packages/transformers/training_args.py:1759: UserWarning: `use_mps_device` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers.`mps` device will be used by default if available similar to the way `cuda` device is used.Therefore, no action from user is required. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "emotion_trainer = setup_trainer('emotion', dataset=ds_emotion, model=emotion_model, tokenizer=emotion_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefan/.local/share/virtualenvs/sentiment_analysis-C3cLsf5i/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb194c755a8443cebaa39a15db4958df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2664, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa6865036cc48fdbcf9fef1ff14e31c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4387818574905396, 'eval_accuracy': 0.4, 'eval_runtime': 0.2204, 'eval_samples_per_second': 68.062, 'eval_steps_per_second': 9.075, 'epoch': 1.0}\n",
      "{'loss': 1.4542, 'learning_rate': 1.2e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eadd4a5650414831aed027cd3e6d9bc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3942002058029175, 'eval_accuracy': 0.4, 'eval_runtime': 0.1956, 'eval_samples_per_second': 76.675, 'eval_steps_per_second': 10.223, 'epoch': 2.0}\n",
      "{'loss': 1.2952, 'learning_rate': 8.000000000000001e-06, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e4ca3cd8a945bc993d45f4c6a33cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.343334436416626, 'eval_accuracy': 0.4, 'eval_runtime': 0.1937, 'eval_samples_per_second': 77.431, 'eval_steps_per_second': 10.324, 'epoch': 3.0}\n",
      "{'loss': 1.1394, 'learning_rate': 4.000000000000001e-06, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba31390dc3744c5d944a7ca75c551ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.330267071723938, 'eval_accuracy': 0.4666666666666667, 'eval_runtime': 0.1954, 'eval_samples_per_second': 76.78, 'eval_steps_per_second': 10.237, 'epoch': 4.0}\n",
      "{'loss': 1.0582, 'learning_rate': 0.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5284cea730c7481a853fdf4b7893d3bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3357529640197754, 'eval_accuracy': 0.4666666666666667, 'eval_runtime': 0.1959, 'eval_samples_per_second': 76.58, 'eval_steps_per_second': 10.211, 'epoch': 5.0}\n",
      "{'train_runtime': 36.6196, 'train_samples_per_second': 16.385, 'train_steps_per_second': 2.048, 'train_loss': 1.4426724370320638, 'epoch': 5.0}\n",
      "Time: 36.62\n",
      "Samples/second: 16.39\n"
     ]
    }
   ],
   "source": [
    "result = emotion_trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fine-tuning-chkp/emotion/checkpoint-60\n"
     ]
    }
   ],
   "source": [
    "print(emotion_trainer.state.best_model_checkpoint)\n",
    "emotion_trainer.save_model('fine-tuning-final/emotion')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theme fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'theme'\n",
    "cols = ds.column_names['train']\n",
    "col_keep = {'text', feature}\n",
    "\n",
    "ds_theme = ds.remove_columns(col_keep.symmetric_difference(cols))\n",
    "ds_theme = ds_theme.rename_column(feature, 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1f6e4f4809043d4bf513fe939179c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a306947a56b949668a63f042e6a7caeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb58cade43b74578bd72babdcb500b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels', 'input_sentence'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "    validate: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels', 'input_sentence'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels', 'input_sentence'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import random\n",
    "\n",
    "\n",
    "theme_tokenizer, theme_config, theme_model = init_model(\"facebook/bart-large-mnli\")\n",
    "# Linear(in_features=1024, out_features=3, bias=True)\n",
    "# {0: 'contradiction', 1: 'neutral', 2: 'entailment'}\n",
    "\n",
    "theme_labels = ['clinical update', 'community', 'question', 'education', 'advocating', 'dissuading', 'other']\n",
    "num_labels = len(theme_labels)\n",
    "template=\"This example is {}.\"\n",
    "\n",
    "def create_input_sequence(sample):\n",
    "    text = sample['text']\n",
    "    label = sample['label'][0]\n",
    "    contradiction_labels = theme_labels[:]\n",
    "    label_idx = contradiction_labels.index(label)\n",
    "    contradiction_labels.pop(label_idx)\n",
    "\n",
    "    encoded_sequence = theme_tokenizer(\n",
    "        text,\n",
    "        [template.format(label)],\n",
    "        # max_length=512,\n",
    "        # padding='max_length', \n",
    "        truncation=True, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    encoded_sequence['labels'] = [2]\n",
    "    encoded_sequence['input_sentence'] = theme_tokenizer.batch_decode(encoded_sequence.input_ids)\n",
    "    return encoded_sequence\n",
    "\n",
    "ds_theme_encoded = ds_theme.map(\n",
    "    create_input_sequence, \n",
    "    batched=True, \n",
    "    batch_size=1,\n",
    "    remove_columns=[\"label\", \"text\"]\n",
    ")\n",
    "\n",
    "ds_theme_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0,\n",
       "  500,\n",
       "  1949,\n",
       "  213,\n",
       "  449,\n",
       "  2013,\n",
       "  116,\n",
       "  50118,\n",
       "  100,\n",
       "  56,\n",
       "  10,\n",
       "  21431,\n",
       "  24904,\n",
       "  626,\n",
       "  11,\n",
       "  1824,\n",
       "  4,\n",
       "  1308,\n",
       "  284,\n",
       "  8,\n",
       "  38,\n",
       "  439,\n",
       "  7,\n",
       "  5,\n",
       "  213,\n",
       "  449,\n",
       "  2013,\n",
       "  1349,\n",
       "  147,\n",
       "  51,\n",
       "  56,\n",
       "  41,\n",
       "  33638,\n",
       "  8,\n",
       "  28445,\n",
       "  9668,\n",
       "  4,\n",
       "  2041,\n",
       "  137,\n",
       "  94,\n",
       "  38,\n",
       "  4024,\n",
       "  10,\n",
       "  213,\n",
       "  449,\n",
       "  2013,\n",
       "  8,\n",
       "  38,\n",
       "  2145,\n",
       "  38,\n",
       "  1705,\n",
       "  17,\n",
       "  27,\n",
       "  90,\n",
       "  269,\n",
       "  2842,\n",
       "  5,\n",
       "  1123,\n",
       "  26965,\n",
       "  8,\n",
       "  20789,\n",
       "  142,\n",
       "  9,\n",
       "  141,\n",
       "  1359,\n",
       "  127,\n",
       "  124,\n",
       "  16,\n",
       "  6,\n",
       "  53,\n",
       "  961,\n",
       "  1493,\n",
       "  198,\n",
       "  162,\n",
       "  115,\n",
       "  2842,\n",
       "  24,\n",
       "  95,\n",
       "  2051,\n",
       "  19,\n",
       "  49,\n",
       "  15145,\n",
       "  18822,\n",
       "  4,\n",
       "  85,\n",
       "  938,\n",
       "  17,\n",
       "  27,\n",
       "  90,\n",
       "  14,\n",
       "  38,\n",
       "  21,\n",
       "  765,\n",
       "  6,\n",
       "  24,\n",
       "  21,\n",
       "  14,\n",
       "  38,\n",
       "  1705,\n",
       "  17,\n",
       "  27,\n",
       "  90,\n",
       "  20789,\n",
       "  4,\n",
       "  6233,\n",
       "  1268,\n",
       "  1493,\n",
       "  655,\n",
       "  2984,\n",
       "  42,\n",
       "  116,\n",
       "  7698,\n",
       "  47,\n",
       "  3068,\n",
       "  213,\n",
       "  449,\n",
       "  7870,\n",
       "  114,\n",
       "  47,\n",
       "  17,\n",
       "  27,\n",
       "  548,\n",
       "  56,\n",
       "  42,\n",
       "  1907,\n",
       "  9,\n",
       "  3012,\n",
       "  116,\n",
       "  2,\n",
       "  2,\n",
       "  713,\n",
       "  1246,\n",
       "  16,\n",
       "  864,\n",
       "  4,\n",
       "  2],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': 2,\n",
       " 'input_sentence': '<s>Ride go kart?\\nI had a spinal fusion done in 2010. My family and I went to the go kart track where they had an arcade and amusement rides. Year before last I drove a go kart and I remember I couldnâ€™t really touch the gas pedal and bend because of how straight my back is, but everyone else around me could touch it just fine with their knees bent. It wasnâ€™t that I was short, it was that I couldnâ€™t bend. Has anyone else ever experienced this? Should you ride go karts if youâ€™ve had this type of surgery?</s></s>This example is question.</s>'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_theme_encoded['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefan/.local/share/virtualenvs/fusion_analysis-xH46poxW/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2412: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,   500,  1949,   213,   449,  2013,   116, 50118,   100,    56,\n",
       "            10, 21431, 24904,   626,    11,  1824,     4,  1308,   284,     8,\n",
       "            38,   439,     7,     5,   213,   449,  2013,  1349,   147,    51,\n",
       "            56,    41, 33638,     8, 28445,  9668,     4,  2041,   137,    94,\n",
       "            38,  4024,    10,   213,   449,  2013,     8,    38,  2145,    38,\n",
       "          1705,    17,    27,    90,   269,  2842,     5,  1123, 26965,     8,\n",
       "         20789,   142,     9,   141,  1359,   127,   124,    16,     6,    53,\n",
       "           961,  1493,   198,   162,   115,  2842,    24,    95,  2051,    19,\n",
       "            49, 15145, 18822,     4,    85,   938,    17,    27,    90,    14,\n",
       "            38,    21,   765,     6,    24,    21,    14,    38,  1705,    17,\n",
       "            27,    90, 20789,     4,  6233,  1268,  1493,   655,  2984,    42,\n",
       "           116,  7698,    47,  3068,   213,   449,  7870,   114,    47,    17,\n",
       "            27,   548,    56,    42,  1907,     9,  3012,   116,     2,     2,\n",
       "           713,  1246,    16,   864,     4,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "premise = ds_theme['train'][0]['text']\n",
    "template= \"This example is {}.\"\n",
    "hypothesis = template.format(ds_theme['train'][0]['label'])\n",
    "\n",
    "# run through model pre-trained on MNLI\n",
    "x = theme_tokenizer(premise, hypothesis, \n",
    "                           truncation_strategy='only_first',\n",
    "        return_tensors='pt')\n",
    "x\n",
    "# logits = theme_model(x.to(device))[0]\n",
    "\n",
    "# # we throw away \"neutral\" (dim 1) and take the probability of\n",
    "# # \"entailment\" (2) as the probability of the label being true \n",
    "# entail_contradiction_logits = logits[:,[0,2]]\n",
    "# probs = entail_contradiction_logits.softmax(dim=1)\n",
    "# prob_label_is_true = probs[:,1]\n",
    "# prob_label_is_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0,\n",
       "  500,\n",
       "  1949,\n",
       "  213,\n",
       "  449,\n",
       "  2013,\n",
       "  116,\n",
       "  50118,\n",
       "  100,\n",
       "  56,\n",
       "  10,\n",
       "  21431,\n",
       "  24904,\n",
       "  626,\n",
       "  11,\n",
       "  1824,\n",
       "  4,\n",
       "  1308,\n",
       "  284,\n",
       "  8,\n",
       "  38,\n",
       "  439,\n",
       "  7,\n",
       "  5,\n",
       "  213,\n",
       "  449,\n",
       "  2013,\n",
       "  1349,\n",
       "  147,\n",
       "  51,\n",
       "  56,\n",
       "  41,\n",
       "  33638,\n",
       "  8,\n",
       "  28445,\n",
       "  9668,\n",
       "  4,\n",
       "  2041,\n",
       "  137,\n",
       "  94,\n",
       "  38,\n",
       "  4024,\n",
       "  10,\n",
       "  213,\n",
       "  449,\n",
       "  2013,\n",
       "  8,\n",
       "  38,\n",
       "  2145,\n",
       "  38,\n",
       "  1705,\n",
       "  17,\n",
       "  27,\n",
       "  90,\n",
       "  269,\n",
       "  2842,\n",
       "  5,\n",
       "  1123,\n",
       "  26965,\n",
       "  8,\n",
       "  20789,\n",
       "  142,\n",
       "  9,\n",
       "  141,\n",
       "  1359,\n",
       "  127,\n",
       "  124,\n",
       "  16,\n",
       "  6,\n",
       "  53,\n",
       "  961,\n",
       "  1493,\n",
       "  198,\n",
       "  162,\n",
       "  115,\n",
       "  2842,\n",
       "  24,\n",
       "  95,\n",
       "  2051,\n",
       "  19,\n",
       "  49,\n",
       "  15145,\n",
       "  18822,\n",
       "  4,\n",
       "  85,\n",
       "  938,\n",
       "  17,\n",
       "  27,\n",
       "  90,\n",
       "  14,\n",
       "  38,\n",
       "  21,\n",
       "  765,\n",
       "  6,\n",
       "  24,\n",
       "  21,\n",
       "  14,\n",
       "  38,\n",
       "  1705,\n",
       "  17,\n",
       "  27,\n",
       "  90,\n",
       "  20789,\n",
       "  4,\n",
       "  6233,\n",
       "  1268,\n",
       "  1493,\n",
       "  655,\n",
       "  2984,\n",
       "  42,\n",
       "  116,\n",
       "  7698,\n",
       "  47,\n",
       "  3068,\n",
       "  213,\n",
       "  449,\n",
       "  7870,\n",
       "  114,\n",
       "  47,\n",
       "  17,\n",
       "  27,\n",
       "  548,\n",
       "  56,\n",
       "  42,\n",
       "  1907,\n",
       "  9,\n",
       "  3012,\n",
       "  116,\n",
       "  2,\n",
       "  2,\n",
       "  713,\n",
       "  1246,\n",
       "  16,\n",
       "  864,\n",
       "  4,\n",
       "  2],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': 2,\n",
       " 'input_sentence': '<s>Ride go kart?\\nI had a spinal fusion done in 2010. My family and I went to the go kart track where they had an arcade and amusement rides. Year before last I drove a go kart and I remember I couldnâ€™t really touch the gas pedal and bend because of how straight my back is, but everyone else around me could touch it just fine with their knees bent. It wasnâ€™t that I was short, it was that I couldnâ€™t bend. Has anyone else ever experienced this? Should you ride go karts if youâ€™ve had this type of surgery?</s></s>This example is question.</s>'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_theme_encoded['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_trainer = setup_trainer('theme', dataset=ds_theme_encoded, model=theme_model, tokenizer=theme_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefan/.local/share/virtualenvs/fusion_analysis-xH46poxW/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8210b71915d46279f21a9744e2ca783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "854fb44d50e84fe4b5954c59feb4fe8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (2, 3) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[39m=\u001b[39m theme_trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m      2\u001b[0m print_summary(result)\n\u001b[1;32m      4\u001b[0m theme_trainer\u001b[39m.\u001b[39msave_model(\u001b[39m'\u001b[39m\u001b[39mfine-tuning-final/theme\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/fusion_analysis-xH46poxW/lib/python3.11/site-packages/transformers/trainer.py:1645\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1640\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1642\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1643\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1644\u001b[0m )\n\u001b[0;32m-> 1645\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1646\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1647\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1648\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1649\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1650\u001b[0m )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/fusion_analysis-xH46poxW/lib/python3.11/site-packages/transformers/trainer.py:2035\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2032\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_training_stop \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   2034\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_epoch_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m-> 2035\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001b[1;32m   2037\u001b[0m \u001b[39mif\u001b[39;00m DebugOption\u001b[39m.\u001b[39mTPU_METRICS_DEBUG \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdebug:\n\u001b[1;32m   2038\u001b[0m     \u001b[39mif\u001b[39;00m is_torch_tpu_available():\n\u001b[1;32m   2039\u001b[0m         \u001b[39m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/fusion_analysis-xH46poxW/lib/python3.11/site-packages/transformers/trainer.py:2321\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2319\u001b[0m         metrics\u001b[39m.\u001b[39mupdate(dataset_metrics)\n\u001b[1;32m   2320\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2321\u001b[0m     metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(ignore_keys\u001b[39m=\u001b[39;49mignore_keys_for_eval)\n\u001b[1;32m   2322\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_report_to_hp_search(trial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2324\u001b[0m \u001b[39m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/fusion_analysis-xH46poxW/lib/python3.11/site-packages/transformers/trainer.py:3053\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3050\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   3052\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3053\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[1;32m   3054\u001b[0m     eval_dataloader,\n\u001b[1;32m   3055\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   3056\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3057\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3058\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   3059\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[1;32m   3060\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[1;32m   3061\u001b[0m )\n\u001b[1;32m   3063\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[1;32m   3064\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmetric_key_prefix\u001b[39m}\u001b[39;00m\u001b[39m_jit_compilation_time\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m output\u001b[39m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/fusion_analysis-xH46poxW/lib/python3.11/site-packages/transformers/trainer.py:3353\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3349\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_metrics(\n\u001b[1;32m   3350\u001b[0m             EvalPrediction(predictions\u001b[39m=\u001b[39mall_preds, label_ids\u001b[39m=\u001b[39mall_labels, inputs\u001b[39m=\u001b[39mall_inputs)\n\u001b[1;32m   3351\u001b[0m         )\n\u001b[1;32m   3352\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 3353\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics(EvalPrediction(predictions\u001b[39m=\u001b[39;49mall_preds, label_ids\u001b[39m=\u001b[39;49mall_labels))\n\u001b[1;32m   3354\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3355\u001b[0m     metrics \u001b[39m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[46], line 8\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(eval_pred)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_metrics\u001b[39m(eval_pred):\n\u001b[1;32m      7\u001b[0m     logits, labels \u001b[39m=\u001b[39m eval_pred\n\u001b[0;32m----> 8\u001b[0m     predictions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49margmax(logits, axis\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m      9\u001b[0m     \u001b[39mreturn\u001b[39;00m metric\u001b[39m.\u001b[39mcompute(predictions\u001b[39m=\u001b[39mpredictions, references\u001b[39m=\u001b[39mlabels)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/fusion_analysis-xH46poxW/lib/python3.11/site-packages/numpy/core/fromnumeric.py:1229\u001b[0m, in \u001b[0;36margmax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m \u001b[39mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[39m(2, 1, 4)\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m kwds \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mkeepdims\u001b[39m\u001b[39m'\u001b[39m: keepdims} \u001b[39mif\u001b[39;00m keepdims \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39m_NoValue \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m-> 1229\u001b[0m \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39margmax\u001b[39;49m\u001b[39m'\u001b[39;49m, axis\u001b[39m=\u001b[39;49maxis, out\u001b[39m=\u001b[39;49mout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/fusion_analysis-xH46poxW/lib/python3.11/site-packages/numpy/core/fromnumeric.py:56\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m bound \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, method, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     55\u001b[0m \u001b[39mif\u001b[39;00m bound \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     58\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/fusion_analysis-xH46poxW/lib/python3.11/site-packages/numpy/core/fromnumeric.py:45\u001b[0m, in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     wrap \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(asarray(obj), method)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m     46\u001b[0m \u001b[39mif\u001b[39;00m wrap:\n\u001b[1;32m     47\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, mu\u001b[39m.\u001b[39mndarray):\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (2, 3) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "result = theme_trainer.train()\n",
    "print_summary(result)\n",
    "\n",
    "theme_trainer.save_model('fine-tuning-final/theme')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
