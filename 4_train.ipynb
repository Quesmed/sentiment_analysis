{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the dataset\n",
    "\n",
    "Pulling manual data classification and placing it into train-validate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_chi = pd.read_csv('data/chi.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chi['selftext'] = df_chi['selftext'].fillna('')\n",
    "df_chi['text'] = df_chi['title'] + '\\n' + df_chi['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_bool = ~df_chi.loc[:, 'negative'].str.startswith('0.')\n",
    "df_manual = df_chi.loc[manual_bool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tone</th>\n",
       "      <th>emotion</th>\n",
       "      <th>theme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>neutral</td>\n",
       "      <td>fear</td>\n",
       "      <td>question</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tone emotion     theme\n",
       "count        26      26        26\n",
       "unique        3       5         5\n",
       "top     neutral    fear  question\n",
       "freq         16      19        16"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_manual.loc[:, ['tone', 'emotion', 'theme']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tone_labels = ['negative', 'neutral', 'positive']\n",
    "emotion_labels = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
    "theme_labels = ['clinical update', 'community', 'question', 'education', 'advocating', 'dissuading', 'other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_test_val_split(df, train_size, val_size, test_size, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits a pandas dataframe into training, validation, and test sets.\n",
    "\n",
    "    Args:\n",
    "    - df: pandas dataframe to split.\n",
    "    - train_size: float between 0 and 1 indicating the proportion of the dataframe to include in the training set.\n",
    "    - val_size: float between 0 and 1 indicating the proportion of the dataframe to include in the validation set.\n",
    "    - test_size: float between 0 and 1 indicating the proportion of the dataframe to include in the test set.\n",
    "    - random_state: int or None, optional (default=42). The seed used by the random number generator.\n",
    "\n",
    "    Returns:\n",
    "    - train_df: pandas dataframe containing the training set.\n",
    "    - val_df: pandas dataframe containing the validation set.\n",
    "    - test_df: pandas dataframe containing the test set.\n",
    "\n",
    "    Raises:\n",
    "    - AssertionError: if the sum of train_size, val_size, and test_size is not equal to 1.\n",
    "    \"\"\"\n",
    "\n",
    "    assert train_size + val_size + test_size == 1, \"Train, validation, and test sizes must add up to 1.\"\n",
    "    \n",
    "    # Split the dataframe into training and test sets\n",
    "    train_df, test_df = train_test_split(df, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # Calculate the size of the validation set relative to the original dataframe\n",
    "    val_ratio = val_size / (1 - test_size)\n",
    "    \n",
    "    # Split the training set into training and validation sets\n",
    "    train_df, val_df = train_test_split(train_df, test_size=val_ratio, random_state=random_state)\n",
    "    \n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['id', 'created_utc', 'text', 'tone', 'emotion', 'theme']\n",
    "train_df, val_df, test_df = train_test_val_split(df_manual.loc[:, cols], 0.8, 0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_path_train = \"data/fine-tune/chi_train.jsonl\"\n",
    "manual_path_validate = \"data/fine-tune/chi_validate.jsonl\"\n",
    "manual_path_test = \"data/fine-tune/chi_test.jsonl\"\n",
    "\n",
    "train_df.to_json(manual_path_train, orient=\"records\", lines=True)\n",
    "val_df.to_json(manual_path_validate, orient=\"records\", lines=True)\n",
    "test_df.to_json(manual_path_test, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /Users/stefan/.cache/huggingface/datasets/json/default-d251bf710f7c6032/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9350e7075f34e7c9f9d7a9edc22817b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "465a0c8627414188ae0ced523faebf25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "887e76a2715e46fa8beeda0c22a77834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6957e1c0de8c4fdbabf3d51fb73c57f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validate split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e199e06c0a7c4070a7d325650d8a1e02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /Users/stefan/.cache/huggingface/datasets/json/default-d251bf710f7c6032/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba5ed5c1c094fef89c0901b2a238162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\n",
    "    'train': manual_path_train,\n",
    "    'validate': manual_path_validate,\n",
    "    'test': manual_path_test\n",
    "}\n",
    "\n",
    "ds = load_dataset(\"json\", data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'created_utc', 'text', 'tone', 'emotion', 'theme'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "    validate: Dataset({\n",
       "        features: ['id', 'created_utc', 'text', 'tone', 'emotion', 'theme'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'created_utc', 'text', 'tone', 'emotion', 'theme'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "def isolate_dataset(ds: Dataset, feature: str):\n",
    "    cols = ds.column_names['train']\n",
    "    col_keep = {'text', feature}\n",
    "    \n",
    "    ds_filter = ds.remove_columns(col_keep.symmetric_difference(cols))\n",
    "    ds_filter = ds_filter.rename_column(feature, 'label')\n",
    "    ds_filter = ds_filter.class_encode_column('label')\n",
    "\n",
    "    return ds_filter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup model and trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "def init_model(model_path: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    config = AutoConfig.from_pretrained(model_path)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "    return (tokenizer, config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, logging\n",
    "from datasets import Dataset\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "def setup_trainer(name: str, dataset: Dataset, model, tokenizer):\n",
    "    logging_steps = len(dataset['train'])\n",
    "    model_name = f\"fine-tuning-chkp/{name}\"\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=model_name,\n",
    "        num_train_epochs=2,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        disable_tqdm=False,\n",
    "        logging_steps=logging_steps,\n",
    "        log_level=\"error\",\n",
    "        use_mps_device=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset['train'],\n",
    "        eval_dataset=dataset['validate'],\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tone_tokenizer, tone_config, tone_model = init_model(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb0aa1447b45423da2fd911bca36737f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe7e2768326149aba5dbd9f3f126cfe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a37b5667ebe848cd808639df6b3e0c3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9baf2fffef24b6799237eb775d660b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8374e395af44409921a8f931588a092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f11518dc459428893691cbb60b9ee30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['negative', 'neutral', 'positive'], id=None),\n",
       " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_tone = isolate_dataset(ds, 'tone')\n",
    "\n",
    "ds_tone = ds_tone.map(\n",
    "  lambda row: tone_tokenizer(row['text'], max_length=512, padding='max_length', truncation=True, return_tensors='pt'), \n",
    "  batched=True,\n",
    "  remove_columns=['text']\n",
    ")\n",
    "\n",
    "ds_tone['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tone_trainer = setup_trainer('tone', dataset=ds_tone, model=tone_model, tokenizer=tone_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefan/.local/share/virtualenvs/fusion_analysis-xH46poxW/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1861ccfe0326428693790955efd6af63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fc488befdfd45499c9462276c0d8a32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1915661096572876, 'eval_accuracy': 0.6666666666666666, 'eval_runtime': 0.0752, 'eval_samples_per_second': 39.888, 'eval_steps_per_second': 13.296, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2176fd3addf84f2e8f5beed64cdc2679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9577839970588684, 'eval_accuracy': 0.6666666666666666, 'eval_runtime': 0.0736, 'eval_samples_per_second': 40.774, 'eval_steps_per_second': 13.591, 'epoch': 2.0}\n",
      "{'train_runtime': 3.6712, 'train_samples_per_second': 10.896, 'train_steps_per_second': 1.634, 'train_loss': 0.49689579010009766, 'epoch': 2.0}\n",
      "Time: 3.67\n",
      "Samples/second: 10.90\n"
     ]
    }
   ],
   "source": [
    "result = tone_trainer.train()\n",
    "print_summary(result)\n",
    "\n",
    "tone_trainer.save_model('fine-tuning-final/tone')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_tokenizer, emotion_config, emotion_model = init_model(\"bhadresh-savani/distilbert-base-uncased-emotion\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbfdce5ec4484b468e3b2ee563f6aebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a067b9d04a704509ae8cc34937c1dc82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f67d87edc594007a67d5b11e785d945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e242e54d216462fa36fdbd7f9dc12c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dade370be2104edb8886a95f715fd40a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc67ba573e6b47e78761768260cb2c93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['fear', 'joy', 'sadness', 'surprise'], id=None),\n",
       " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# emotion_labels = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
    "\n",
    "ds_emotion = isolate_dataset(ds, 'emotion')\n",
    "\n",
    "ds_emotion = ds_emotion.map(\n",
    "  lambda row: emotion_tokenizer(row['text'], max_length=512, padding='max_length', truncation=True, return_tensors='pt'), \n",
    "  batched=True,\n",
    "  remove_columns=['text']\n",
    ")\n",
    "\n",
    "ds_emotion['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_trainer = setup_trainer('emotion', dataset=ds_emotion, model=emotion_model, tokenizer=emotion_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefan/.local/share/virtualenvs/fusion_analysis-xH46poxW/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae5c6241b1f543c58bee711c7e445864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d173e109842148b0823d06735ef5e948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3068583309650421, 'eval_accuracy': 1.0, 'eval_runtime': 0.0651, 'eval_samples_per_second': 46.053, 'eval_steps_per_second': 15.351, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "739723bb88ba4237a8b85c302637e963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5239936113357544, 'eval_accuracy': 0.6666666666666666, 'eval_runtime': 0.048, 'eval_samples_per_second': 62.56, 'eval_steps_per_second': 20.853, 'epoch': 2.0}\n",
      "{'train_runtime': 2.1537, 'train_samples_per_second': 18.572, 'train_steps_per_second': 2.786, 'train_loss': 2.852828025817871, 'epoch': 2.0}\n",
      "Time: 2.15\n",
      "Samples/second: 18.57\n"
     ]
    }
   ],
   "source": [
    "result = emotion_trainer.train()\n",
    "print_summary(result)\n",
    "\n",
    "emotion_trainer.save_model('fine-tuning-final/emotion')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theme fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'theme'\n",
    "cols = ds.column_names['train']\n",
    "col_keep = {'text', feature}\n",
    "\n",
    "ds_theme = ds.remove_columns(col_keep.symmetric_difference(cols))\n",
    "ds_theme = ds_theme.rename_column(feature, 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1f6e4f4809043d4bf513fe939179c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a306947a56b949668a63f042e6a7caeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb58cade43b74578bd72babdcb500b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels', 'input_sentence'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "    validate: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels', 'input_sentence'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels', 'input_sentence'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import random\n",
    "\n",
    "\n",
    "theme_tokenizer, theme_config, theme_model = init_model(\"facebook/bart-large-mnli\")\n",
    "# Linear(in_features=1024, out_features=3, bias=True)\n",
    "# {0: 'contradiction', 1: 'neutral', 2: 'entailment'}\n",
    "\n",
    "theme_labels = ['clinical update', 'community', 'question', 'education', 'advocating', 'dissuading', 'other']\n",
    "num_labels = len(theme_labels)\n",
    "template=\"This example is {}.\"\n",
    "\n",
    "def create_input_sequence(sample):\n",
    "    text = sample['text']\n",
    "    label = sample['label'][0]\n",
    "    contradiction_labels = theme_labels[:]\n",
    "    label_idx = contradiction_labels.index(label)\n",
    "    contradiction_labels.pop(label_idx)\n",
    "\n",
    "    encoded_sequence = theme_tokenizer(\n",
    "        text,\n",
    "        [template.format(label)],\n",
    "        # max_length=512,\n",
    "        # padding='max_length', \n",
    "        truncation=True, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    encoded_sequence['labels'] = [2]\n",
    "    encoded_sequence['input_sentence'] = theme_tokenizer.batch_decode(encoded_sequence.input_ids)\n",
    "    return encoded_sequence\n",
    "\n",
    "ds_theme_encoded = ds_theme.map(\n",
    "    create_input_sequence, \n",
    "    batched=True, \n",
    "    batch_size=1,\n",
    "    remove_columns=[\"label\", \"text\"]\n",
    ")\n",
    "\n",
    "ds_theme_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0,\n",
       "  500,\n",
       "  1949,\n",
       "  213,\n",
       "  449,\n",
       "  2013,\n",
       "  116,\n",
       "  50118,\n",
       "  100,\n",
       "  56,\n",
       "  10,\n",
       "  21431,\n",
       "  24904,\n",
       "  626,\n",
       "  11,\n",
       "  1824,\n",
       "  4,\n",
       "  1308,\n",
       "  284,\n",
       "  8,\n",
       "  38,\n",
       "  439,\n",
       "  7,\n",
       "  5,\n",
       "  213,\n",
       "  449,\n",
       "  2013,\n",
       "  1349,\n",
       "  147,\n",
       "  51,\n",
       "  56,\n",
       "  41,\n",
       "  33638,\n",
       "  8,\n",
       "  28445,\n",
       "  9668,\n",
       "  4,\n",
       "  2041,\n",
       "  137,\n",
       "  94,\n",
       "  38,\n",
       "  4024,\n",
       "  10,\n",
       "  213,\n",
       "  449,\n",
       "  2013,\n",
       "  8,\n",
       "  38,\n",
       "  2145,\n",
       "  38,\n",
       "  1705,\n",
       "  17,\n",
       "  27,\n",
       "  90,\n",
       "  269,\n",
       "  2842,\n",
       "  5,\n",
       "  1123,\n",
       "  26965,\n",
       "  8,\n",
       "  20789,\n",
       "  142,\n",
       "  9,\n",
       "  141,\n",
       "  1359,\n",
       "  127,\n",
       "  124,\n",
       "  16,\n",
       "  6,\n",
       "  53,\n",
       "  961,\n",
       "  1493,\n",
       "  198,\n",
       "  162,\n",
       "  115,\n",
       "  2842,\n",
       "  24,\n",
       "  95,\n",
       "  2051,\n",
       "  19,\n",
       "  49,\n",
       "  15145,\n",
       "  18822,\n",
       "  4,\n",
       "  85,\n",
       "  938,\n",
       "  17,\n",
       "  27,\n",
       "  90,\n",
       "  14,\n",
       "  38,\n",
       "  21,\n",
       "  765,\n",
       "  6,\n",
       "  24,\n",
       "  21,\n",
       "  14,\n",
       "  38,\n",
       "  1705,\n",
       "  17,\n",
       "  27,\n",
       "  90,\n",
       "  20789,\n",
       "  4,\n",
       "  6233,\n",
       "  1268,\n",
       "  1493,\n",
       "  655,\n",
       "  2984,\n",
       "  42,\n",
       "  116,\n",
       "  7698,\n",
       "  47,\n",
       "  3068,\n",
       "  213,\n",
       "  449,\n",
       "  7870,\n",
       "  114,\n",
       "  47,\n",
       "  17,\n",
       "  27,\n",
       "  548,\n",
       "  56,\n",
       "  42,\n",
       "  1907,\n",
       "  9,\n",
       "  3012,\n",
       "  116,\n",
       "  2,\n",
       "  2,\n",
       "  713,\n",
       "  1246,\n",
       "  16,\n",
       "  864,\n",
       "  4,\n",
       "  2],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': 2,\n",
       " 'input_sentence': '<s>Ride go kart?\\nI had a spinal fusion done in 2010. My family and I went to the go kart track where they had an arcade and amusement rides. Year before last I drove a go kart and I remember I couldn’t really touch the gas pedal and bend because of how straight my back is, but everyone else around me could touch it just fine with their knees bent. It wasn’t that I was short, it was that I couldn’t bend. Has anyone else ever experienced this? Should you ride go karts if you’ve had this type of surgery?</s></s>This example is question.</s>'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_theme_encoded['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefan/.local/share/virtualenvs/fusion_analysis-xH46poxW/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2412: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,   500,  1949,   213,   449,  2013,   116, 50118,   100,    56,\n",
       "            10, 21431, 24904,   626,    11,  1824,     4,  1308,   284,     8,\n",
       "            38,   439,     7,     5,   213,   449,  2013,  1349,   147,    51,\n",
       "            56,    41, 33638,     8, 28445,  9668,     4,  2041,   137,    94,\n",
       "            38,  4024,    10,   213,   449,  2013,     8,    38,  2145,    38,\n",
       "          1705,    17,    27,    90,   269,  2842,     5,  1123, 26965,     8,\n",
       "         20789,   142,     9,   141,  1359,   127,   124,    16,     6,    53,\n",
       "           961,  1493,   198,   162,   115,  2842,    24,    95,  2051,    19,\n",
       "            49, 15145, 18822,     4,    85,   938,    17,    27,    90,    14,\n",
       "            38,    21,   765,     6,    24,    21,    14,    38,  1705,    17,\n",
       "            27,    90, 20789,     4,  6233,  1268,  1493,   655,  2984,    42,\n",
       "           116,  7698,    47,  3068,   213,   449,  7870,   114,    47,    17,\n",
       "            27,   548,    56,    42,  1907,     9,  3012,   116,     2,     2,\n",
       "           713,  1246,    16,   864,     4,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "premise = ds_theme['train'][0]['text']\n",
    "template= \"This example is {}.\"\n",
    "hypothesis = template.format(ds_theme['train'][0]['label'])\n",
    "\n",
    "# run through model pre-trained on MNLI\n",
    "x = theme_tokenizer(premise, hypothesis, \n",
    "                           truncation_strategy='only_first',\n",
    "        return_tensors='pt')\n",
    "x\n",
    "# logits = theme_model(x.to(device))[0]\n",
    "\n",
    "# # we throw away \"neutral\" (dim 1) and take the probability of\n",
    "# # \"entailment\" (2) as the probability of the label being true \n",
    "# entail_contradiction_logits = logits[:,[0,2]]\n",
    "# probs = entail_contradiction_logits.softmax(dim=1)\n",
    "# prob_label_is_true = probs[:,1]\n",
    "# prob_label_is_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0,\n",
       "  500,\n",
       "  1949,\n",
       "  213,\n",
       "  449,\n",
       "  2013,\n",
       "  116,\n",
       "  50118,\n",
       "  100,\n",
       "  56,\n",
       "  10,\n",
       "  21431,\n",
       "  24904,\n",
       "  626,\n",
       "  11,\n",
       "  1824,\n",
       "  4,\n",
       "  1308,\n",
       "  284,\n",
       "  8,\n",
       "  38,\n",
       "  439,\n",
       "  7,\n",
       "  5,\n",
       "  213,\n",
       "  449,\n",
       "  2013,\n",
       "  1349,\n",
       "  147,\n",
       "  51,\n",
       "  56,\n",
       "  41,\n",
       "  33638,\n",
       "  8,\n",
       "  28445,\n",
       "  9668,\n",
       "  4,\n",
       "  2041,\n",
       "  137,\n",
       "  94,\n",
       "  38,\n",
       "  4024,\n",
       "  10,\n",
       "  213,\n",
       "  449,\n",
       "  2013,\n",
       "  8,\n",
       "  38,\n",
       "  2145,\n",
       "  38,\n",
       "  1705,\n",
       "  17,\n",
       "  27,\n",
       "  90,\n",
       "  269,\n",
       "  2842,\n",
       "  5,\n",
       "  1123,\n",
       "  26965,\n",
       "  8,\n",
       "  20789,\n",
       "  142,\n",
       "  9,\n",
       "  141,\n",
       "  1359,\n",
       "  127,\n",
       "  124,\n",
       "  16,\n",
       "  6,\n",
       "  53,\n",
       "  961,\n",
       "  1493,\n",
       "  198,\n",
       "  162,\n",
       "  115,\n",
       "  2842,\n",
       "  24,\n",
       "  95,\n",
       "  2051,\n",
       "  19,\n",
       "  49,\n",
       "  15145,\n",
       "  18822,\n",
       "  4,\n",
       "  85,\n",
       "  938,\n",
       "  17,\n",
       "  27,\n",
       "  90,\n",
       "  14,\n",
       "  38,\n",
       "  21,\n",
       "  765,\n",
       "  6,\n",
       "  24,\n",
       "  21,\n",
       "  14,\n",
       "  38,\n",
       "  1705,\n",
       "  17,\n",
       "  27,\n",
       "  90,\n",
       "  20789,\n",
       "  4,\n",
       "  6233,\n",
       "  1268,\n",
       "  1493,\n",
       "  655,\n",
       "  2984,\n",
       "  42,\n",
       "  116,\n",
       "  7698,\n",
       "  47,\n",
       "  3068,\n",
       "  213,\n",
       "  449,\n",
       "  7870,\n",
       "  114,\n",
       "  47,\n",
       "  17,\n",
       "  27,\n",
       "  548,\n",
       "  56,\n",
       "  42,\n",
       "  1907,\n",
       "  9,\n",
       "  3012,\n",
       "  116,\n",
       "  2,\n",
       "  2,\n",
       "  713,\n",
       "  1246,\n",
       "  16,\n",
       "  864,\n",
       "  4,\n",
       "  2],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': 2,\n",
       " 'input_sentence': '<s>Ride go kart?\\nI had a spinal fusion done in 2010. My family and I went to the go kart track where they had an arcade and amusement rides. Year before last I drove a go kart and I remember I couldn’t really touch the gas pedal and bend because of how straight my back is, but everyone else around me could touch it just fine with their knees bent. It wasn’t that I was short, it was that I couldn’t bend. Has anyone else ever experienced this? Should you ride go karts if you’ve had this type of surgery?</s></s>This example is question.</s>'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_theme_encoded['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_trainer = setup_trainer('theme', dataset=ds_theme_encoded, model=theme_model, tokenizer=theme_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefan/.local/share/virtualenvs/fusion_analysis-xH46poxW/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8210b71915d46279f21a9744e2ca783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "854fb44d50e84fe4b5954c59feb4fe8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (2, 3) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[39m=\u001b[39m theme_trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m      2\u001b[0m print_summary(result)\n\u001b[1;32m      4\u001b[0m theme_trainer\u001b[39m.\u001b[39msave_model(\u001b[39m'\u001b[39m\u001b[39mfine-tuning-final/theme\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/fusion_analysis-xH46poxW/lib/python3.11/site-packages/transformers/trainer.py:1645\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1640\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1642\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1643\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1644\u001b[0m )\n\u001b[0;32m-> 1645\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1646\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1647\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1648\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1649\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1650\u001b[0m )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/fusion_analysis-xH46poxW/lib/python3.11/site-packages/transformers/trainer.py:2035\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2032\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_training_stop \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   2034\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_epoch_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m-> 2035\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001b[1;32m   2037\u001b[0m \u001b[39mif\u001b[39;00m DebugOption\u001b[39m.\u001b[39mTPU_METRICS_DEBUG \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdebug:\n\u001b[1;32m   2038\u001b[0m     \u001b[39mif\u001b[39;00m is_torch_tpu_available():\n\u001b[1;32m   2039\u001b[0m         \u001b[39m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/fusion_analysis-xH46poxW/lib/python3.11/site-packages/transformers/trainer.py:2321\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2319\u001b[0m         metrics\u001b[39m.\u001b[39mupdate(dataset_metrics)\n\u001b[1;32m   2320\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2321\u001b[0m     metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(ignore_keys\u001b[39m=\u001b[39;49mignore_keys_for_eval)\n\u001b[1;32m   2322\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_report_to_hp_search(trial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2324\u001b[0m \u001b[39m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/fusion_analysis-xH46poxW/lib/python3.11/site-packages/transformers/trainer.py:3053\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3050\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   3052\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3053\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[1;32m   3054\u001b[0m     eval_dataloader,\n\u001b[1;32m   3055\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   3056\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3057\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3058\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   3059\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[1;32m   3060\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[1;32m   3061\u001b[0m )\n\u001b[1;32m   3063\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[1;32m   3064\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmetric_key_prefix\u001b[39m}\u001b[39;00m\u001b[39m_jit_compilation_time\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m output\u001b[39m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/fusion_analysis-xH46poxW/lib/python3.11/site-packages/transformers/trainer.py:3353\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3349\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_metrics(\n\u001b[1;32m   3350\u001b[0m             EvalPrediction(predictions\u001b[39m=\u001b[39mall_preds, label_ids\u001b[39m=\u001b[39mall_labels, inputs\u001b[39m=\u001b[39mall_inputs)\n\u001b[1;32m   3351\u001b[0m         )\n\u001b[1;32m   3352\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 3353\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics(EvalPrediction(predictions\u001b[39m=\u001b[39;49mall_preds, label_ids\u001b[39m=\u001b[39;49mall_labels))\n\u001b[1;32m   3354\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3355\u001b[0m     metrics \u001b[39m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[46], line 8\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(eval_pred)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_metrics\u001b[39m(eval_pred):\n\u001b[1;32m      7\u001b[0m     logits, labels \u001b[39m=\u001b[39m eval_pred\n\u001b[0;32m----> 8\u001b[0m     predictions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49margmax(logits, axis\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m      9\u001b[0m     \u001b[39mreturn\u001b[39;00m metric\u001b[39m.\u001b[39mcompute(predictions\u001b[39m=\u001b[39mpredictions, references\u001b[39m=\u001b[39mlabels)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/fusion_analysis-xH46poxW/lib/python3.11/site-packages/numpy/core/fromnumeric.py:1229\u001b[0m, in \u001b[0;36margmax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m \u001b[39mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[39m(2, 1, 4)\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m kwds \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mkeepdims\u001b[39m\u001b[39m'\u001b[39m: keepdims} \u001b[39mif\u001b[39;00m keepdims \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39m_NoValue \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m-> 1229\u001b[0m \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39margmax\u001b[39;49m\u001b[39m'\u001b[39;49m, axis\u001b[39m=\u001b[39;49maxis, out\u001b[39m=\u001b[39;49mout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/fusion_analysis-xH46poxW/lib/python3.11/site-packages/numpy/core/fromnumeric.py:56\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m bound \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, method, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     55\u001b[0m \u001b[39mif\u001b[39;00m bound \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     58\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/fusion_analysis-xH46poxW/lib/python3.11/site-packages/numpy/core/fromnumeric.py:45\u001b[0m, in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     wrap \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(asarray(obj), method)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m     46\u001b[0m \u001b[39mif\u001b[39;00m wrap:\n\u001b[1;32m     47\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, mu\u001b[39m.\u001b[39mndarray):\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (2, 3) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "result = theme_trainer.train()\n",
    "print_summary(result)\n",
    "\n",
    "theme_trainer.save_model('fine-tuning-final/theme')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fusion_analysis-xH46poxW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
